{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Класифікатор спаму: підготовка корпусу SpamAssassin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Відкрита база листів [SpamAssassin Public Corpus](https://spamassassin.apache.org/old/publiccorpus/) складається з кількох наборів текстових файлів, де кожен файл — дамп email-протоколу для одного листа. Тому, початкова мета — перетворити ці дані в формат, легший для моделювання.\n",
    "\n",
    "Ми виконаємо такі операції:\n",
    "\n",
    "1. Залишимо тільки тіла листів (email body), і вилучимо всі службові заголовки email-протоколів.\n",
    "2. Приберемо всі шумові слова (stop words) — артиклі, прийменники тощо.\n",
    "3. Замінимо всі URL, числа та email-адреси на службові слова `HTTPADDRESS`, `NUMBER`, `EMAILADDRESS`.\n",
    "4. Всі слова, що залишилися, нормалізуємо за допомогою [стемінгу (stemming)](https://en.wikipedia.org/wiki/Stemming).\n",
    "5. Збережемо слова кожного листа у вигляді одного великого JSON-масиву."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "import glob\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm as progressbar\n",
    "# import tqdm import tqdm.notebook as progressbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Завантажимо необхідні мовні моделі та словники:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# nltk.download(\"stopwords\")\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "stemmer = nltk.stem.snowball.EnglishStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Розділення листів на слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Відокремлюємо тіла листів на ділимо їх на слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_email_filename = re.compile(r\"[0-9a-f]{5}\\.[0-9a-f]{32}\", re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_url = re.compile(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", flags=re.MULTILINE | re.UNICODE)\n",
    "re_email = re.compile(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\", flags=re.MULTILINE | re.UNICODE)\n",
    "re_number = re.compile(r\"\\b\\d+\\b\", flags=re.MULTILINE | re.UNICODE)\n",
    "re_hash = re.compile(r\"[0-9a-f]{32}\", flags=re.MULTILINE | re.UNICODE)\n",
    "re_non_word = re.compile(r\"\\W+\", flags=re.MULTILINE | re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_body_to_words(body, stopwords, stemmer):\n",
    "    # Convert to lowercase.\n",
    "    body = body.lower()\n",
    "\n",
    "    # Replace URLs.\n",
    "    body = re_url.sub(\"HTTPADDRESS\", body)\n",
    "\n",
    "    # Replace email addresses.\n",
    "    body = re_email.sub(\"EMAILADDRESS\", body)\n",
    "\n",
    "    # Replace hashes.\n",
    "    body = re_hash.sub(\"HASH\", body)\n",
    "    \n",
    "    # Replace numbers.\n",
    "    body = re_number.sub(\"NUMBER\", body)\n",
    "\n",
    "    # Remove non-word characters (punctuation, spacers, etc.)\n",
    "    body = re_non_word.sub(\" \", body)\n",
    "\n",
    "    # Remove trailing spaces.\n",
    "    body = body.strip()\n",
    "\n",
    "    # Remove stopwords and stem the remaining words.\n",
    "    words = [stemmer.stem(word) for word in body.split() if word not in stopwords]\n",
    "    \n",
    "    # Remove technical-looking words (base64, underscores, etc.) words.\n",
    "    words = [word for word in words if len(word) <= 20 and not \"_\" in word and not any(ch.isdigit() for ch in word)]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_emails_in_folders(folders, filename_regex, stopwords, stemmer):\n",
    "    emails = []\n",
    "    \n",
    "    # Iterate through every folder in our dataset.\n",
    "    for folder in progressbar(folders, desc=\"Folders\"):\n",
    "        email_filenames = [\n",
    "            filename\n",
    "            for filename in sorted(os.listdir(folder))\n",
    "            if filename_regex.match(os.path.basename(filename))\n",
    "        ]\n",
    "\n",
    "        # Each email is kept as a single file, iterate through them.\n",
    "        for file in progressbar(email_filenames, desc=os.path.basename(folder)):\n",
    "            email_path = os.path.join(folder, file)\n",
    "            \n",
    "            with open(email_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as email_file:\n",
    "                msg = email.message_from_file(email_file)\n",
    "                if not msg.is_multipart():\n",
    "                    body = msg.get_payload()\n",
    "                    words = email_body_to_words(body, stopwords, stemmer)\n",
    "                    emails.append(words)\n",
    "    \n",
    "    return emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"C:\\\\Users\\\\user\\\\ML_Basecamp\\\\Naive Bayes\\\\data\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26bc1ed88b2045ddb8f20311b3cb1928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Folders', max=3, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175d7be649b142c5a19f896900cbcc40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='easy_ham', max=2500, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf64cb04e23433ba88900f358f5724b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='easy_ham_2', max=1400, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e50208a48f84f6b9b643a95cb689bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='hard_ham', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emails_tokenized_ham = tokenize_emails_in_folders(\n",
    "    [\n",
    "        folder + \"easy_ham\",\n",
    "        folder + \"easy_ham_2\",\n",
    "        folder + \"hard_ham\"\n",
    "    ],\n",
    "    re_email_filename,\n",
    "    stopwords,\n",
    "    stemmer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c7e5220b7514c73bd7d087d4efee4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Folders', max=2, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff4e714cc984c06831d154b64025e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='spam', max=500, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "396b218b4ac5400bb5e15845bf8a8804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='spam_2', max=1396, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emails_tokenized_spam = tokenize_emails_in_folders(\n",
    "    [\n",
    "        folder + \"spam\",\n",
    "        folder + \"spam_2\"\n",
    "    ],\n",
    "    re_email_filename,\n",
    "    stopwords,\n",
    "    stemmer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Ham emails:   3952\n",
      "# Spam emails:  1590\n"
     ]
    }
   ],
   "source": [
    "print(\"# Ham emails:  \", len(emails_tokenized_ham))\n",
    "print(\"# Spam emails: \", len(emails_tokenized_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_json_file(obj, filename):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:/Students/Spam Classifier/Spam data/emails-tokenized-ham.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-0a82008a0529>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msave_as_json_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memails_tokenized_ham\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"D:/Students/Spam Classifier/Spam data/emails-tokenized-ham.json\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msave_as_json_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memails_tokenized_spam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"D:/Students/Spam Classifier/Spam data/emails-tokenized-spam.json\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-f67bad5dba48>\u001b[0m in \u001b[0;36msave_as_json_file\u001b[1;34m(obj, filename)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msave_as_json_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/Students/Spam Classifier/Spam data/emails-tokenized-ham.json'"
     ]
    }
   ],
   "source": [
    "save_as_json_file(emails_tokenized_ham, \"D:/Students/Spam Classifier/Spam data/emails-tokenized-ham.json\")\n",
    "save_as_json_file(emails_tokenized_spam, \"D:/Students/Spam Classifier/Spam data/emails-tokenized-spam.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Побудова словника"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша мета — присвоїти порядковий номер кожному унікальному слову в базі."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отримуємо множину всіх унікальних слів:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_set = set(itertools.chain(*emails_tokenized_ham)).union(set(itertools.chain(*emails_tokenized_spam)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формуємо з неї словник: _cлово_ -> _номер_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    word: index\n",
    "    for index, word in enumerate(sorted(list(vocab_set)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length: 34133\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary length:\", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перевіримо наш словних на кількох випадкових словах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy        3953\n",
      "watch      32213\n",
      "discount   7616\n"
     ]
    }
   ],
   "source": [
    "for word in [\"buy\", \"watch\", \"discount\"]:\n",
    "    print(word.ljust(10), vocab[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Збережемо словник у файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_json_file(vocab, folder + \"vocab.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Огляд результатів"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перевіримо, як наш метод очищення листів працює на прикладі довільного листа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_email = email.message_from_string(open(\\\n",
    "                        \"D:/Students/Spam Classifier/Spam data/easy_ham/01306.01273f7d32eaabde7b20f220e13eb927\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello,\n",
      "\n",
      "Has anyone made a working source RPM for dvd::rip for Red Hat 8.0?\n",
      "Matthias has a spec file on the site for 0.46, and there are a couple of\n",
      "spec files lying around on the dvd::rip website, including one I patched\n",
      "a while ago, but it appears that the Makefile automatically generated is\n",
      "trying to install the Perl libraries into the system's, and also at the\n",
      "moment dvd::rip needs to be called with PERLIO=stdio as it seems to not\n",
      "work with PerlIO on RH8's Perl.\n",
      "\n",
      "Not too sure what the cleanest way to fix this is - anyone working on\n",
      "this?\n",
      "\n",
      "Thanks,\n",
      "\n",
      "-- \n",
      "MichÃ¨l Alexandre Salim\n",
      "Web:\t\thttp://salimma.freeshell.org\n",
      "GPG/PGP key:\thttp://salimma.freeshell.org/files/crypto/publickey.asc\n",
      "\n",
      "__________________________________________________\n",
      "Do You Yahoo!?\n",
      "Everything you'll ever need on one web page\n",
      "from News and Sport to Email and Music Charts\n",
      "http://uk.my.yahoo.com\n",
      "\n",
      "_______________________________________________\n",
      "RPM-List mailing list <RPM-List@freshrpms.net>\n",
      "http://lists.freshrpms.net/mailman/listinfo/rpm-list\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_body = sample_email.get_payload()\n",
    "print(sample_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'anyon', 'made', 'work', 'sourc', 'rpm', 'dvd', 'rip', 'red', 'hat', 'number', 'number', 'matthia', 'spec', 'file', 'site', 'number', 'number', 'coupl', 'spec', 'file', 'lie', 'around', 'dvd', 'rip', 'websit', 'includ', 'one', 'patch', 'ago', 'appear', 'makefil', 'automat', 'generat', 'tri', 'instal', 'perl', 'librari', 'system', 'also', 'moment', 'dvd', 'rip', 'need', 'call', 'perlio', 'stdio', 'seem', 'work', 'perlio', 'perl', 'sure', 'cleanest', 'way', 'fix', 'anyon', 'work', 'thank', 'michã', 'l', 'alexandr', 'salim', 'web', 'httpaddress', 'gpg', 'pgp', 'key', 'httpaddress', 'yahoo', 'everyth', 'ever', 'need', 'one', 'web', 'page', 'news', 'sport', 'email', 'music', 'chart', 'httpaddress', 'rpm', 'list', 'mail', 'list', 'emailaddress', 'httpaddress']\n"
     ]
    }
   ],
   "source": [
    "print(email_body_to_words(sample_body, stopwords, stemmer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
